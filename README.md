# Fast And Accurate Deep Network Learning Using Exponential Linear Units(ELUs)
 
We introduce the ”exponential linear unit” (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LRe- LUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values and used a Convolution Neural Network with MNIST as our standard database for the comparison of the activation functions.

# Tasks 

1. Understanding the mathematics and concepts of the paper.
2. Implementing a neural network using sigmoid as activation function.
3. Implementing a neural network using ReLU and ELU as activation functions.
4. Comparing the classification accuracies of all three models,

